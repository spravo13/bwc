#!/bin/bash
VISITED_URLS_FILE="$HOME/crawler_visited_urls"
     
EXPR_URL_MATCH="http://[^'\"]\+\.html"
     
WGET_TIMEOUT=10
WGET_RETRIES=0
     
RUNTIME=30

START_TIME=`date +%s`
     
if [[ "$CRAWLER_RECURSION" == "" ]]; then
    rm -f $VISITED_URLS_FILE
    export CRAWLER_RECURSION=1
     
    touch $VISITED_URLS_FILE
else
    let CRAWLER_RECURSION++
fi
     
while getopts e:t:s OPTION; do
    case "$OPTION" in
        *)
            SUMMARY=1
        ;;
    esac
done
 
while shift; do
    if [[ "$1" == *://* ]]; then
        URL=$1
        break
    fi
done
     
if [[ "$DEBUG" != "" ]]; then
    echo ">> $URL / T=$RUNTIME / CRAWLER_RECURSION=$CRAWLER_RECURSION"
fi
     
if [ "$URL" == "" ]; then
    echo "Syntax: $0 [-e expression] [-t runtime] [-s] URL"
    exit 1
fi
     
TMP_PAGE=`mktemp`
wget $URL -t $WGET_RETRIES -T $WGET_TIMEOUT -qO $TMP_PAGE
echo $URL >> $VISITED_URLS_FILE
     
if [ $? != 0 ]; then
    rm $TMP_PAGE
    exit $?
fi
     
echo $URL 
     
URL_LIST=`grep -o $EXPR_URL_MATCH $TMP_PAGE`
     
rm $TMP_PAGE
     
for u in $URL_LIST; do
    CURRENT_TIME=`date +%s`
    if [[ ! `grep -o $u $VISITED_URLS_FILE` ]]; then
        $0 -t $(($RUNTIME - ($CURRENT_TIME - $START_TIME))) -e $EXPRESSION $u
    fi
done
     
if [[ "$CRAWLER_RECURSION" == "1" ]]; then
    unset CRAWLER_RECURSION
fi
     
let CRAWLER_RECURSION--

